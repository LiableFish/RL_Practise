{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7CyocjBJ1SP"
   },
   "source": [
    "## Unequal Game solution with CCEM method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AABSYH3vJ1SR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cZE6eLK2MdR"
   },
   "source": [
    "# Calculating optimal guaranteed results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9eLwv6LY4xn"
   },
   "source": [
    "Let's find guaranteed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKfpwnUFMYsK"
   },
   "source": [
    "Definitions\n",
    "\n",
    "\\begin{gather*}\n",
    "\\Gamma^u = \\min_{u(\\cdot)}\\max_{v(\\cdot)} \\gamma(u,\\, v), \\quad \n",
    "\\Gamma^v = \\max_{v(\\cdot)}\\min_{u(\\cdot)} \\gamma(u,\\, v), \\\\\n",
    "\\tilde{\\Gamma}^u = \\min_{u(\\cdot,\\, \\cdot)}\\max_{v(\\cdot,\\, \\cdot)} \\gamma(u, v), \\quad \n",
    "\\tilde{\\Gamma}^v = \\max_{v(\\cdot,\\, \\cdot)}\\min_{u(\\cdot,\\, \\cdot)} \\gamma(u, v), \\\\\\\\\n",
    "\\Gamma^v \\leqslant \\tilde{\\Gamma}^v \\leqslant  \\tilde{\\Gamma}^v \\leqslant \\Gamma^u.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjyRmxcvY_pQ"
   },
   "source": [
    "Our differential game model\n",
    "\n",
    "\\begin{gather*}\n",
    "\\begin{cases}\n",
    "\\dot{x} = u(t,\\,x) - v(t,\\,x), \\quad t \\in [0, 2],\\\\\n",
    "x(0) = 1, \\\\\n",
    "|u| \\leqslant 2, \\quad |v| \\leqslant 1.\n",
    "\\end{cases}, \\\\ \\\\\n",
    "\\gamma(u,\\, v) = x^2(2) = \\left(1 + \\int\\limits_0^2 u(t, x)\\,dt - \\int\\limits_0^2 v(t, x)\\,dt \\right)^2 = (1 +U(x) + V(x))^2, \\\\|U| \\leqslant 4, \\quad |V| \\leqslant 2, \\\\ \\\\\n",
    "\\Gamma^u = 4, \\quad \\Gamma^v = \\tilde{\\Gamma}^v = \\tilde{\\Gamma}^v = 0.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAd2UkykbgJ0"
   },
   "source": [
    "Compute $\\Gamma^u$\n",
    "\n",
    "\\begin{gather*}\n",
    "\\max_V \\gamma(U,\\, V) = \\max\\left\\{\\gamma(U,\\, 2),\\, \\gamma(U,\\, -2)\\right\\}, \\\\ \\\\\n",
    "\\gamma(U, 2) - \\gamma(U, -2) = -8(1+U).\n",
    "\\end{gather*}\n",
    "\n",
    "Case 1:\n",
    "\\begin{gather*}\n",
    "\\max_V \\gamma(U,\\, V) = \\gamma(U,\\, 2) \\; \\Longrightarrow \\; U \\leqslant -1 \\; \\Longrightarrow \\; \\Gamma^u = \\min_U \\gamma(U\\,2) = \\gamma(-1, 2) = 4.\n",
    "\\end{gather*}\n",
    "\n",
    "Case 2:\n",
    "\\begin{gather*}\n",
    "\\max_V \\gamma(U,\\, V) = \\gamma(U,\\, -2) \\; \\Longrightarrow \\; U \\geqslant -1 \\; \\Longrightarrow \\; \\Gamma^u = \\min_U \\gamma(U\\,-2) = \\gamma(-1, -2) = 4.\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4J4HcicfuGC"
   },
   "source": [
    "Compute $\\Gamma^v$\n",
    "\n",
    "\\begin{gather*}\n",
    "\\frac{\\partial}{\\partial U} \\gamma(U,\\,V) = 2(1 + U - V) = 0 \\;\\Longrightarrow\\; U = V - 1 \\Longrightarrow\\; \\Gamma^v = \\max_{V}\\gamma(V-1,\\,V) = 0\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20rbj4TTllV8"
   },
   "source": [
    "Compute $\\tilde{\\Gamma}^u$\n",
    "\n",
    "\\begin{gather*}\n",
    "\\forall U, V \\quad \\tilde{\\Gamma}^u \\geqslant 0, \\\\ \\\\\n",
    "U(x) = - 4 \\cdot \\text{sign}(x) \\;\\Longrightarrow\\; \\gamma(U,\\, V) = \\max_{V}(1 - 4 \\cdot \\text{sign}(x) - V(x))^2\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VhvN61Hitl5J"
   },
   "source": [
    "Compute $\\tilde{\\Gamma}^v$\n",
    "\n",
    "\\begin{gather*}\n",
    "0 = \\Gamma^v \\leqslant \\tilde{\\Gamma}^v \\leqslant \\tilde{\\Gamma}^u = 0 \\; \\Longrightarrow \\; \\tilde{\\Gamma}^v = 0.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKQpP3ao2VR6"
   },
   "source": [
    "# Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5YRCc2MJ1SU"
   },
   "source": [
    "## Implement environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fj46-jGRJ1SV"
   },
   "outputs": [],
   "source": [
    "class UnequalGame:\n",
    "    \"\"\"Unequal game environment implementation for RL practise\"\"\"\n",
    "\n",
    "    def __init__(self, initial_x=1, dt=0.005, terminal_time=2, u_action_max=2, v_action_max=1):\n",
    "        \"\"\"Create an environment\n",
    "        :param initial_x: starting point for u_agent\n",
    "        :param dt: time rate\n",
    "        :param terminal_time: stopping time\n",
    "        :param u_action_max: maximum action value for u_agent\n",
    "        :param v_action_max: maximum action value for v_agent\n",
    "        \"\"\"\n",
    "        self.u_action_max = u_action_max\n",
    "        self.v_action_max = v_action_max\n",
    "        self.terminal_time = terminal_time\n",
    "        self.dt = dt\n",
    "        self.initial_x = initial_x\n",
    "        self.state = self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment state for a new game session\n",
    "        :return: starting state\n",
    "        \"\"\"\n",
    "        self.state = np.array([0, self.initial_x])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, u_action, v_action):\n",
    "        \"\"\"Generate a new environment state under the actions of agents\n",
    "        :param u_action: u_agent action\n",
    "        :param v_action: v_agent action\n",
    "        :return: new state, reward, done flag, None\n",
    "        \"\"\"\n",
    "        t, x = self.state\n",
    "        x = x + (u_action - v_action) * self.dt\n",
    "        t += self.dt\n",
    "        self.state = np.array([t, x])\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if t >= self.terminal_time:\n",
    "            reward = x ** 2\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01OM_i5PJ1SX"
   },
   "source": [
    "## Implement agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7S80KcE1J1SY"
   },
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \"\"\"Neural network for an agent\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        \"\"\"Create new network\n",
    "        :param input_shape: input data shape\n",
    "        :param output_shape: output data shape\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(input_shape[0], 50)\n",
    "        self.linear_2 = torch.nn.Linear(50, 30)\n",
    "        self.linear_3 = torch.nn.Linear(30, output_shape[0])\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tang = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Network step\n",
    "        :param input_: input data\n",
    "        :return: network output result\n",
    "        \"\"\"\n",
    "        hidden = self.relu(self.linear_1(input_))\n",
    "        hidden = self.relu(self.linear_2(hidden))\n",
    "        output = self.tang(self.linear_3(hidden))\n",
    "        return output\n",
    "\n",
    "\n",
    "class CCEMAgent(torch.nn.Module):\n",
    "    \"\"\"Continuous cross-entropy method agent implementation\"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, action_max, reward_param=1, percentile_param=70, noise_decrease=0.98,\n",
    "                 tau=1e-2, learning_rate=1e-2, n_learning_per_fit=16, mini_batch_size=200):\n",
    "        \"\"\"Create new agent\n",
    "        :param state_shape: environment's state shape\n",
    "        :param action_shape: agent's action shape\n",
    "        :param action_max: maximum action value\n",
    "        :param reward_param: equal to 1 if agent wants to maximize reward otherwise -1\n",
    "        :param percentile_param: percentile to get elite sessions\n",
    "        :param noise_decrease: noise decrease value\n",
    "        :param tau: network weights updating rate\n",
    "        :param learning_rate: learning rate for gradient descent method\n",
    "        :param n_learning_per_fit: number of network updating weights iterations per fit\n",
    "        :param mini_batch_size: count of elements to sample to mini-batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.action_max = np.abs(action_max)\n",
    "        self.reward_param = reward_param\n",
    "        self.percentile_param = percentile_param\n",
    "        self.noise_decrease = noise_decrease\n",
    "        self.noise_threshold = 1\n",
    "        self.min_noise_threshold = 0.1\n",
    "        self.tau = tau\n",
    "        self.n_learning_per_fit = n_learning_per_fit\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.network = Network(state_shape, action_shape)\n",
    "        self.optimizer = torch.optim.Adam(params=self.network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_action(self, state, test=False):\n",
    "        \"\"\"Get an action by current state\n",
    "        :param state: current environment state\n",
    "        :param test: if True noise will not be added and will be otherwise\n",
    "        :return: predicted action\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        predicted_action = self.network(state).detach().numpy() * self.action_max\n",
    "        if not test:\n",
    "            noise = self.noise_threshold * np.random.uniform(low=-self.action_max, high=self.action_max)\n",
    "            predicted_action = np.clip(predicted_action + noise, -self.action_max, self.action_max)\n",
    "        return predicted_action\n",
    "\n",
    "    def get_elite_states_and_actions(self, sessions):\n",
    "        \"\"\"Select sessions with the most or least reward by percentile\n",
    "        :param sessions: list of sessions to choose elite ones from\n",
    "        :return: elite states, elite actions\n",
    "        \"\"\"\n",
    "        total_rewards = [session['total_reward'] for session in sessions]\n",
    "        reward_threshold = np.percentile(total_rewards, self.percentile_param)\n",
    "        \n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for session in sessions:\n",
    "            if self.reward_param * (session['total_reward'] - reward_threshold) > 0:\n",
    "                elite_states.extend(session['states'])\n",
    "                elite_actions.extend(session[f'{self}actions'])\n",
    "\n",
    "        return torch.FloatTensor(elite_states), torch.FloatTensor(elite_actions)\n",
    "\n",
    "    def learn_network(self, loss):\n",
    "        \"\"\"Update network weights by optimize loss\n",
    "        :param loss: loss function to optimize\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        old_network = deepcopy(self.network)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        for new_parameter, old_parameter in zip(self.network.parameters(), old_network.parameters()):\n",
    "            new_parameter.data.copy_(self.tau * new_parameter + (1 - self.tau) * old_parameter)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self, sessions):\n",
    "        \"\"\"Fitting process using mini-batches\n",
    "        :param sessions: sessions to fit on\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        elite_states, elite_actions = self.get_elite_states_and_actions(sessions)\n",
    "\n",
    "        for _ in range(self.n_learning_per_fit):\n",
    "            mini_batch_idxs = np.random.choice(range(elite_states.shape[0]), size=self.mini_batch_size)\n",
    "            mini_batch_states = elite_states[mini_batch_idxs]\n",
    "            mini_batch_actions = elite_actions[mini_batch_idxs]\n",
    "\n",
    "            predicted_action = self.network(mini_batch_states) * self.action_max\n",
    "            loss = torch.mean((predicted_action - mini_batch_actions) ** 2)\n",
    "            self.learn_network(loss)\n",
    "\n",
    "        if self.noise_threshold > self.min_noise_threshold:\n",
    "            self.noise_threshold *= self.noise_decrease\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"An agent string representation to define if it is u_agent or v_agent\n",
    "        :return: string representation\n",
    "        \"\"\"\n",
    "        return 'u_' if self.reward_param == -1 else 'v_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiiiimQO6Sip"
   },
   "source": [
    "## Generate sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrjBLPuDJ1Sb"
   },
   "outputs": [],
   "source": [
    "def generate_session(first_agent, second_agent, env, n_iter_update, test=False):\n",
    "    \"\"\"Generate session on environment with agents\n",
    "    :param first_agent: first agent (u_agent by default)\n",
    "    :param second_agent: second agent (v_agent by default)\n",
    "    :param env: environment\n",
    "    :param n_iter_update: number of iterations between getting new actions\n",
    "    :param test: if True first agent will not add noise in get_action method\n",
    "    :return: session dict wit states, first agent actions, second agent actions and total rewards\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    first_agent_actions = []\n",
    "    second_agent_actions = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iteration = 0\n",
    "    while not done:\n",
    "        if iteration % n_iter_update == 0:\n",
    "            first_agent_action = first_agent.get_action(state, test=test)\n",
    "            second_agent_action = second_agent.get_action(state)\n",
    "            states.append(state)\n",
    "            first_agent_actions.append(first_agent_action)\n",
    "            second_agent_actions.append(second_agent_action)\n",
    "        actions = (first_agent_action[0], second_agent_action[0]) if str(first_agent) == 'u_' else (second_agent_action[0], first_agent_action[0])\n",
    "        next_state, reward, done, _ = env.step(*actions)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        iteration += 1\n",
    "\n",
    "    return {'states': states,\n",
    "            f'{first_agent}actions': first_agent_actions,\n",
    "            f'{second_agent}actions': second_agent_actions,\n",
    "            'total_reward': total_reward}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAwsBQXn6X_F"
   },
   "source": [
    "## Fit one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EK6wNInF6Xad"
   },
   "outputs": [],
   "source": [
    "def fit_epoch(u_agent, v_agent, env, n_sessions, n_iter_update, test):\n",
    "    \"\"\"Fit agents during an one epoch\n",
    "    :param u_agent: agent that wants to minimize reward\n",
    "    :param v_agent: agent that wants to maximize reward\n",
    "    :param env: environment\n",
    "    :param n_sessions: number of sessions\n",
    "    :param n_iter_update: number of iterations between getting new actions\n",
    "    :param test: if True u_agent will not be fitted\n",
    "    :return: mean total reward over sessions\n",
    "    \"\"\"\n",
    "    #sessions = [generate_session(u_agent, v_agent, env, test=test) for _ in range(n_sessions)]\n",
    "    with torch.multiprocessing.Pool(torch.multiprocessing.cpu_count()) as pool:\n",
    "        sessions = pool.starmap(generate_session, [(u_agent, v_agent, env, n_iter_update, test) for _ in range(n_sessions)])\n",
    "    mean_reward = np.mean([session['total_reward'] for session in sessions])\n",
    "    if not test:\n",
    "        u_agent.fit(sessions)\n",
    "    v_agent.fit(sessions)\n",
    "    return mean_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fTW3tWbHyiu"
   },
   "source": [
    "## Test agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUU1ytZhHca5"
   },
   "outputs": [],
   "source": [
    "def test_agent(u_agent, env, n_epochs, n_sessions, n_iter_update, epsilon):\n",
    "    \"\"\"Test u_agent by fit a new v_agent\n",
    "    :param u_agent: agent to test (must be u_agent type)\n",
    "    :param env: environment\n",
    "    :param n_epochs: number of epochs to fit\n",
    "    :param n_sessions: number of sessions for one epoch\n",
    "    :param n_iter_update: number of iterations between getting new actions\n",
    "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
    "    :return: test total rewards\n",
    "    \"\"\"\n",
    "    v_agent = CCEMAgent((2,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)\n",
    "    _, rewards, _ = fit_agents(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update, epsilon, test=True)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzV4HKqsJ1Sb"
   },
   "source": [
    "## Fit agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GY6XooSIHbeE"
   },
   "outputs": [],
   "source": [
    "def fit_agents(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update,\n",
    "               epsilon, n_iter_debug=0, test=False):\n",
    "    \"\"\"Fit both agent together during several epochs\n",
    "    :param u_agent: agent that wants to minimize reward\n",
    "    :param v_agent: agent that wants to maximize reward\n",
    "    :param env: environment\n",
    "    :param n_epochs: number of epochs to fit\n",
    "    :param n_sessions: number of sessions for one epoch\n",
    "        :param n_iter_update: number of iterations between getting new actions\n",
    "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
    "    :param n_iter_debug: number of iteration between tests\n",
    "    :param test: if True u_agent will not be fitted\n",
    "    :return: u_agent, mean total rewards, test total rewards\n",
    "    \"\"\"\n",
    "    last_mean_reward = 0\n",
    "    mean_rewards = []\n",
    "    test_rewards = []\n",
    "    epoch = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        mean_reward = fit_epoch(u_agent, v_agent, env, n_sessions=n_sessions, n_iter_update=n_iter_update, test=test)\n",
    "        mean_rewards.append(mean_reward)\n",
    "        print(f'epoch: {epoch}, mean reward: {mean_reward}')\n",
    "        if np.abs(last_mean_reward - mean_reward) < epsilon:\n",
    "            break\n",
    "        last_mean_reward = mean_reward\n",
    "\n",
    "        if n_iter_debug and (epoch + 1) % n_iter_debug == 0:\n",
    "            print('\\n{:-^50}\\n'.format('TEST BEGIN'))\n",
    "            test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=epsilon))\n",
    "            print('\\n{:-^50}\\n'.format('TEST END'))             \n",
    "\n",
    "    return u_agent, np.array(mean_rewards), np.array(test_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ts0iEOAPwNbf"
   },
   "source": [
    "## Fit agents one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zWWu0hLwXVU"
   },
   "outputs": [],
   "source": [
    "def fit_agents_one_by_one(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update,\n",
    "                          n_iter_for_fit, epsilon, n_iter_debug=0):\n",
    "    \"\"\"Fit agents ony by one during several epochs.\n",
    "    During fix number of iterations one agent will be fitted while the other will not.\n",
    "    :param u_agent: agent that wants to minimize reward\n",
    "    :param v_agent: agent that wants to maximize reward\n",
    "    :param env: environment\n",
    "    :param n_epochs: number of epochs to fit\n",
    "    :param n_sessions: number of sessions for one epoch\n",
    "    :param n_iter_update: number of iterations between getting new actions\n",
    "    :param n_iter_for_fit: number of iterations between agent switching\n",
    "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
    "    :param n_iter_debug: number of iteration between tests\n",
    "    :return: u_agent, mean  total rewards, test total rewards\n",
    "    \"\"\"\n",
    "    last_mean_reward = 0\n",
    "    mean_rewards = []\n",
    "    test_rewards = []\n",
    "    fitting_agent = u_agent\n",
    "    awaiting_agent = v_agent\n",
    "    epoch = 0\n",
    "    stop = False\n",
    "\n",
    "    while not stop and epoch < n_epochs:\n",
    "\n",
    "        for _ in range(n_iter_for_fit):\n",
    "\n",
    "            mean_reward = fit_epoch(awaiting_agent, fitting_agent, env, n_sessions=n_sessions, n_iter_update=n_iter_update, test=True)\n",
    "            mean_rewards.append(mean_reward)\n",
    "            print(f'epoch: {epoch}, current agent: {fitting_agent}, mean reward: {mean_reward}')\n",
    "            if np.abs(last_mean_reward - mean_reward) < epsilon:\n",
    "                stop = True\n",
    "                break\n",
    "            last_mean_reward = mean_reward\n",
    "\n",
    "            if n_iter_debug and (epoch + 1) % n_iter_debug == 0:\n",
    "                print('\\n{:-^50}\\n'.format('TEST BEGIN'))\n",
    "                test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=epsilon))\n",
    "                print('\\n{:-^50}\\n'.format('TEST END'))\n",
    "\n",
    "            epoch += 1\n",
    "            if epoch >= n_epochs:\n",
    "                break\n",
    "\n",
    "        print('\\n')\n",
    "        awaiting_agent, fitting_agent = fitting_agent, awaiting_agent\n",
    "\n",
    "    return u_agent, np.array(mean_rewards), np.array(test_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY5KIG4n5qAi"
   },
   "source": [
    "## Fit random agent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "To2h6mLx7cSE"
   },
   "outputs": [],
   "source": [
    "def fit_random_agent_pairs(u_agents, v_agents, env, n_pairs, n_epochs, n_sessions, n_iter_update, n_iter_debug=0):\n",
    "    \"\"\" Fit random pairs of u_ and v_agents\n",
    "    :param u_agent: agent that wants to minimize reward\n",
    "    :param v_agent: agent that wants to maximize reward\n",
    "    :param env: environment\n",
    "    :param n_pairs: number of pairs to fit\n",
    "    :param n_epochs: number of epochs for one pair fit\n",
    "    :param n_sessions: number of sessions for one epoch\n",
    "    :param n_iter_update: number of iterations between getting new actions\n",
    "    :param n_iter_debug: number of iteration between tests\n",
    "    :return: u_agent that will have minimum test total reward, mean total rewards for u_agents, test total rewards\n",
    "    \"\"\"\n",
    "    u_agents_mean_rewards = [[] for _ in range(len(u_agents))]\n",
    "    test_rewards = []\n",
    "\n",
    "    for i in range(n_pairs):\n",
    "        u_agent_idx = np.random.choice(len(u_agents))\n",
    "        v_agent_idx = np.random.choice(len(v_agents))\n",
    "        print(f'PAIR {i + 1} OF {n_pairs}')\n",
    "        print('\\n{:-^50}\\n'.format(f'U_AGENT_{u_agent_idx} VS V_AGENT_{v_agent_idx}'))\n",
    "        _, mean_rewards, _ = fit_agents(u_agents[u_agent_idx], v_agents[v_agent_idx],\n",
    "                                        env=env, n_epochs=n_epochs, n_sessions=n_sessions,\n",
    "                                        n_iter_update=n_iter_update, epsilon=-1, n_iter_debug=0)\n",
    "        print('\\n{:-^50}\\n'.format(''))\n",
    "\n",
    "        u_agents_mean_rewards[u_agent_idx].append(mean_rewards.min())\n",
    "\n",
    "    for i, u_agent in enumerate(u_agents):\n",
    "        print(f'\\nTESTING U_AGENT_{i}\\n')\n",
    "        test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=-1))\n",
    "\n",
    "    best_u_agent_idx = np.argmin([test.max() for test in test_rewards])\n",
    "    print(f'\\nBest agent is {best_u_agent_idx}, its test reward is {np.min(test_rewards[best_u_agent_idx])}\\n')\n",
    "    return u_agents[best_u_agent_idx], np.array(u_agents_mean_rewards), np.array(test_rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wR_cLRa2r7A"
   },
   "source": [
    "## Plot mean rewards by epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pI2rrGrK2w6l"
   },
   "outputs": [],
   "source": [
    "def plot_mean_rewards(mean_rewards, method_name, axes, test_rewards=None):\n",
    "    \"\"\"Plot mean rewards\n",
    "    :param mean_rewards: rewards to plot\n",
    "    :param method_name: fitting method name\n",
    "    :param test_rewards: test rewards to plot\n",
    "    \"\"\"\n",
    "    if test_rewards is not None:\n",
    "        ax1, ax2 = axes\n",
    "    else:\n",
    "        ax1 = axes\n",
    "    ax1.plot(range(len(mean_rewards)), mean_rewards)\n",
    "    ax1.set_xlabel('Номер эпохи')\n",
    "    ax1.set_ylabel('Средний total reward')\n",
    "    ax1.set_title(f'Среднее значение total reward для {method_name}')\n",
    "    ax1.grid()\n",
    "    if test_rewards is not None:\n",
    "        ax2.set_xlabel('Номер эпохи')\n",
    "        ax2.set_ylabel('Тестовый total reward')\n",
    "        ax2.set_title(f'Тестовое значение total reward для {method_name}')\n",
    "        for i, test_reward in enumerate(test_rewards):\n",
    "            ax2.plot(range(len(test_reward)), test_reward, label=f'После {int((i + 1) * len(mean_rewards) / len(test_rewards))} эпох')\n",
    "        ax2.legend(loc='upper left')\n",
    "        ax2.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnequalGame()\n",
    "u_agent = CCEMAgent((2,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
    "v_agent = CCEMAgent((2,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sessions = [generate_session(u_agent, v_agent, env, 5, test=False) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_session(u_agent, v_agent, env, 5, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_count = multiprocessing.cpu_count()\n",
    "process_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.multiprocessing.Pool(process_count) as pool:\n",
    "    %timeit pool.starmap(generate_session, [(u_agent, v_agent, env, 5, False) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgUptSYv2FIK"
   },
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDwDnGJD5qa9"
   },
   "source": [
    "## Deafualt fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXE4PAqKJ1Se"
   },
   "outputs": [],
   "source": [
    "env = UnequalGame()\n",
    "u_agent = CCEMAgent((2,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
    "v_agent = CCEMAgent((2,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_bpTd8JerT8E",
    "outputId": "48f8ec49-003f-4613-8232-f5b9aed0c78a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "u_fit_agent, mean_rewards, test_rewards \\\n",
    "= fit_agents(u_agent, v_agent, env, n_epochs=1000, n_sessions=100, n_iter_update=20, epsilon=-1, n_iter_debug=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "orw6Xmrgrdww",
    "outputId": "50277c86-742e-4a14-fc36-d56fa476bea1"
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plot_mean_rewards(mean_rewards, method_name='CCME метода', axes=axes, test_rewards=test_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsIq2Y3V5njb"
   },
   "source": [
    "## Fit one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OW1QxXVg6gYp"
   },
   "outputs": [],
   "source": [
    "u_agent_one_by_one = CCEMAgent((2,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
    "v_agent_one_by_one = CCEMAgent((2,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4DV2oVjprieg",
    "outputId": "c5a18281-2c86-46f8-999c-d5bdb731eb6d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "u_fit_agent_one_by_one, mean_rewards_one_by_one, test_rewards_one_by_one =\\\n",
    "fit_agents_one_by_one(u_agent_one_by_one, v_agent_one_by_one, env, \\\n",
    "                       n_epochs=1000, n_sessions=100, n_iter_for_fit=50, n_iter_update=20, epsilon=-1, n_iter_debug=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "eydMMs5KryCi",
    "outputId": "148945db-cc04-4acf-ad48-a81fc57b3bb9"
   },
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "c = 'b'\n",
    "l = 0\n",
    "\n",
    "for rewards in np.array_split(mean_rewards_one_by_one, 20):\n",
    "    ax1.plot(range(l, l + len(rewards)), rewards, color=c)\n",
    "    l = l + len(rewards)\n",
    "    c = 'b' if c=='g' else 'g'\n",
    "ax1.set_xlabel('Номер эпохи')\n",
    "ax1.set_ylabel('Средний total reward')\n",
    "ax1.set_title('Среднее значение total reward\\nдля метода попеременного обучения')\n",
    "ax1.legend(labels=['u_agent', 'v_agent'])\n",
    "ax1.grid()\n",
    "\n",
    "ax2.set_xlabel('Номер эпохи')\n",
    "ax2.set_ylabel('Тестовый total reward')\n",
    "ax2.set_title('Тестовое значение total reward\\nдля метода попеременного обучения')\n",
    "\n",
    "for i, test_reward in enumerate(test_rewards_one_by_one):\n",
    "    ax2.plot(range(len(test_reward)), test_reward, label=f'После {int((i + 1) * len(mean_rewards_one_by_one) / len(test_rewards_one_by_one))} эпох')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "348CMrbE5U5Y"
   },
   "source": [
    "## Fit random pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZQDoMwYUcf_"
   },
   "outputs": [],
   "source": [
    "u_agents = [CCEMAgent((2,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\\\n",
    "            for _ in range(5)]\n",
    "v_agents = [CCEMAgent((2,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)\\\n",
    "            for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lgcN3yChr2CW",
    "outputId": "580bd277-fe60-48de-8ca3-9c4eb828d453"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "u_fit_random_agent, mean_rewards_random, test_rewards_random = \\\n",
    "fit_random_agent_pairs(u_agents, v_agents, env,\\\n",
    "                       n_pairs=50, n_epochs=20, n_sessions=100,\\\n",
    "                       n_iter_update=20, n_iter_debug=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "WBU50PYPsb_W",
    "outputId": "812cbfec-9d0d-4f18-fa33-7dcb997ec31f"
   },
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for i, rewards in enumerate(mean_rewards_random):\n",
    "    ax1.plot(range(len(rewards)), rewards, label=f'u_agent_{i}')\n",
    "\n",
    "for i, test in enumerate(test_rewards_random):\n",
    "    ax2.plot(range(len(test)), test, label=f'u_agent_{i}')\n",
    "\n",
    "ax1.set_xlabel('Номер пары')\n",
    "ax1.set_ylabel('Средний total reward')\n",
    "ax1.set_title('Среднее значение total reward\\nдля метода случайных пар')\n",
    "ax1.grid()\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2.set_xlabel('Номер эпохи')\n",
    "ax2.set_ylabel('Тестовый total reward')\n",
    "ax2.set_title('Тестовое значение total reward\\nдля каждого агента')\n",
    "ax2.grid()\n",
    "ax2.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_ht-gKrb4OJF",
    "outputId": "2d421658-7865-438d-ca13-d67925c04ec0"
   },
   "outputs": [],
   "source": [
    "test_rewards_random.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9PYm_lX2h-H"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MM6uZqenAZoj"
   },
   "source": [
    "Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Zfw7hoQAbna"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZqdAcMD1pOpi"
   },
   "outputs": [],
   "source": [
    "def get_test_data(test_rewards, test_rewards_one_by_one):\n",
    "    data = np.vstack((test_rewards.max(axis=1),\\\n",
    "                      test_rewards_one_by_one.max(axis=1),\\\n",
    "                     test_rewards_random.max(axis=1)))\n",
    "    return pd.DataFrame(data=data,\\\n",
    "                        columns=[f'После {i} эпох' for i in range(200, 1001, 200)],\\\n",
    "                        index=['CCME', 'Метод отложенного обучения', 'Метод случайных пар'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GH1TID87ogDz"
   },
   "outputs": [],
   "source": [
    "dt01_data = get_test_data(test_rewards, test_rewards_one_by_one, test_rewards_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "MZaNX_lBpmLA",
    "outputId": "ccf9f0c3-35a8-42fc-de7c-66bc33cd1eb5"
   },
   "outputs": [],
   "source": [
    "display(dt01_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrkkxFDSJ1Sn"
   },
   "source": [
    "## Test default u_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yzC4Sb2QJ1Sq",
    "outputId": "464d1f11-17f9-45c7-8f44-0507232343f5"
   },
   "outputs": [],
   "source": [
    "test_all_rewards = test_agent(u_fit_agent, env, n_epochs=500, n_sessions=100, epsilon=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "i7ZITn34J1Ss",
    "outputId": "22c4d838-c8c9-479d-f713-30b6ac681f67"
   },
   "outputs": [],
   "source": [
    "plot_mean_rewards(test_all_rewards, method_name='default test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aR-GlF-f0Yg"
   },
   "source": [
    "## Test u_agent_one_by_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KZorei8v7Np4",
    "outputId": "418ea011-e011-49e1-bf22-8c0d4d0966fe"
   },
   "outputs": [],
   "source": [
    "test_all_rewards_one_by_one = test_agent(u_fit_agent_one_by_one, env, n_epochs=300, n_sessions=100, epsilon=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "cXK3jgOj7QN1",
    "outputId": "d8759fe4-0ba9-408c-c611-82cab84e5de9"
   },
   "outputs": [],
   "source": [
    "plot_mean_rewards(test_all_rewards_one_by_one, method_name='test one-by-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X50sgCCUef2"
   },
   "source": [
    "## Test u_agent_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qu2e5hPZUg9k",
    "outputId": "ce5ebe4a-3fb6-4099-e9d4-4ba9cf5a5b5c"
   },
   "outputs": [],
   "source": [
    "test_all_rewards_random = test_agent(u_fit_random_agent, env, n_epochs=300, n_sessions=100, epsilon=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "36jJd-5Hflhl",
    "outputId": "4272e694-fc0e-456d-c029-a9e508580807"
   },
   "outputs": [],
   "source": [
    "plot_mean_rewards(test_all_rewards_random, method_name='test random pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QhKKwIDFw_lQ",
    "outputId": "25548366-52b8-42b5-bf14-84364ff253ee"
   },
   "outputs": [],
   "source": [
    "u_fit_random_agent == u_agents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uM8TN3rA89SA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "UnequalGameImplementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
