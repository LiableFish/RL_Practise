{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unequal Game solution with CCEM method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnequalGame:\n",
    "\n",
    "    def __init__(self, initial_x=1, dt=0.005, terminal_time=2, u_action_max=2, v_action_max=1):\n",
    "        self.u_action_max = u_action_max\n",
    "        self.v_action_max = v_action_max\n",
    "        self.terminal_time = terminal_time\n",
    "        self.dt = dt\n",
    "        self.initial_x = initial_x\n",
    "        self.state = self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([0, self.initial_x])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, u_action, v_action):\n",
    "        t, x = self.state\n",
    "        x = x + (u_action - v_action) * self.dt\n",
    "        t += self.dt\n",
    "        self.state = np.array([t, x])\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if t >= self.terminal_time:\n",
    "            reward = x ** 2\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(input_shape[0], 50)\n",
    "        self.linear_2 = torch.nn.Linear(50, 30)\n",
    "        self.linear_3 = torch.nn.Linear(30, output_shape[0])\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tang = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        hidden = self.relu(self.linear_1(input_))\n",
    "        hidden = self.relu(self.linear_2(hidden))\n",
    "        output = self.tang(self.linear_3(hidden))\n",
    "        return output\n",
    "\n",
    "\n",
    "class CCEMAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, action_max, reward_param=1, percentile_param=70, noise_decrease=0.98,\n",
    "                 tau=1e-2, learning_rate=1e-2, n_learning_per_fit=16):\n",
    "        super().__init__()\n",
    "        self.action_max = np.abs(action_max)\n",
    "        self.reward_param = reward_param  # equal to 1 if agent wants to maximize reward otherwise -1\n",
    "        self.percentile_param = percentile_param\n",
    "        self.noise_decrease = noise_decrease\n",
    "        self.noise_threshold = 1\n",
    "        self.min_noise_threshold = 0.1\n",
    "        self.tau = tau\n",
    "        self.n_learning_per_fit = n_learning_per_fit\n",
    "        self.network = Network(state_shape, action_shape)\n",
    "        self.optimizer = torch.optim.Adam(params=self.network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        predicted_action = self.network(state).detach().numpy()\n",
    "        noise = self.noise_threshold * np.random.uniform(low=-1, high=1)\n",
    "        action = np.clip(predicted_action + noise, -self.action_max, self.action_max)\n",
    "        return action\n",
    "\n",
    "    def get_elite_states_and_actions(self, sessions, prefix):\n",
    "        \"\"\"\n",
    "          Select sessions with the most or least reward\n",
    "          by percentile\n",
    "        \"\"\"\n",
    "        total_rewards = [session['total_reward'] for session in sessions]\n",
    "        reward_threshold = np.percentile(total_rewards, self.percentile_param)\n",
    "\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for session in sessions:\n",
    "            if self.reward_param * (session['total_reward'] - reward_threshold) > 0:\n",
    "                elite_states.extend(session['states'])\n",
    "                elite_actions.extend(session[f'{prefix}actions'])\n",
    "\n",
    "        return torch.FloatTensor(elite_states), torch.FloatTensor(elite_actions)\n",
    "\n",
    "    def learn_network(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        old_network = deepcopy(self.network)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        for new_parameter, old_parameter in zip(self.network.parameters(), old_network.parameters()):\n",
    "            new_parameter.data.copy_((1 - self.tau) * new_parameter + self.tau * old_parameter)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fit(self, sessions, prefix=''):\n",
    "        elite_states, elite_actions = self.get_elite_states_and_actions(sessions, prefix=prefix)\n",
    "\n",
    "        for _ in range(self.n_learning_per_fit):\n",
    "            predicted_action = self.network(elite_states)\n",
    "            loss = torch.mean((predicted_action - elite_actions) ** 2)\n",
    "            self.learn_network(loss)\n",
    "\n",
    "        if self.noise_threshold > self.min_noise_threshold:\n",
    "            self.noise_threshold *= self.noise_decrease\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(u_agent, v_agent, env, t_max=200):\n",
    "    \"\"\"\n",
    "    Generate session on environment with agent\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    u_actions = []\n",
    "    v_actions = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        u_action = u_agent.get_action(state)\n",
    "        v_action = v_agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(u_action[0], v_action[0])\n",
    "        states.append(state)\n",
    "        u_actions.append(u_action)\n",
    "        v_actions.append(v_action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return {'states': states, 'u_actions': u_actions, 'v_actions': v_actions, 'total_reward': total_reward}\n",
    "\n",
    "\n",
    "def train_agents(u_agent, v_agent, env, n_epochs, n_sessions, epsilon, test=False):\n",
    "    last_mean_reward = 0\n",
    "    mean_rewards = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sessions = [generate_session(u_agent, v_agent, env) for _ in range(n_sessions)]\n",
    "        mean_reward = np.mean([session['total_reward'] for session in sessions])\n",
    "        if not test:\n",
    "            u_agent.fit(sessions, prefix='u_')\n",
    "        v_agent.fit(sessions, prefix='v_')\n",
    "        mean_rewards.append(mean_reward)\n",
    "        print('epoch: {0}, mean reward: {1}'.format(epoch, mean_reward))\n",
    "        # if epoch > 0:\n",
    "        #    clear_output(True)\n",
    "        #    plt.plot(range(len(mean_rewards)), mean_rewards)\n",
    "        #    plt.show()\n",
    "        if np.abs(last_mean_reward - mean_reward) < epsilon:\n",
    "            break\n",
    "        last_mean_reward = mean_reward\n",
    "        \n",
    "    return np.array(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnequalGame()\n",
    "u_agent = CCEMAgent((2,), (1,), percentile_param=30, action_max=env.u_action_max, reward_param=-1)\n",
    "v_agent = CCEMAgent((2,), (1,), percentile_param=70, action_max=env.v_action_max, reward_param=1)\n",
    "n_epochs = 100\n",
    "n_sessions = 100\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, mean reward: 0.3351087898760319\n",
      "epoch: 1, mean reward: 0.46109226925460683\n",
      "epoch: 2, mean reward: 0.5056249891227123\n",
      "epoch: 3, mean reward: 0.5204197324090394\n",
      "epoch: 4, mean reward: 0.5625592511373864\n",
      "epoch: 5, mean reward: 0.6083741915019903\n",
      "epoch: 6, mean reward: 0.635795213083863\n",
      "epoch: 7, mean reward: 0.6102800791928601\n",
      "epoch: 8, mean reward: 0.5727067590023898\n",
      "epoch: 9, mean reward: 0.5227293809481021\n",
      "epoch: 10, mean reward: 0.4606627875835747\n",
      "epoch: 11, mean reward: 0.430565317711096\n",
      "epoch: 12, mean reward: 0.4049933114357043\n",
      "epoch: 13, mean reward: 0.3777978850006759\n",
      "epoch: 14, mean reward: 0.3506407173768569\n",
      "epoch: 15, mean reward: 0.34042857352315126\n",
      "epoch: 16, mean reward: 0.3424351156831399\n",
      "epoch: 17, mean reward: 0.3460571928285636\n",
      "epoch: 18, mean reward: 0.3736614338788269\n",
      "epoch: 19, mean reward: 0.41264125599221657\n",
      "epoch: 20, mean reward: 0.4469050544761336\n",
      "epoch: 21, mean reward: 0.5086008940904685\n",
      "epoch: 22, mean reward: 0.5329486013959746\n",
      "epoch: 23, mean reward: 0.5461564315099554\n",
      "epoch: 24, mean reward: 0.5424608299411616\n",
      "epoch: 25, mean reward: 0.5547987859198226\n",
      "epoch: 26, mean reward: 0.5445441656465638\n",
      "epoch: 27, mean reward: 0.5604580867964465\n",
      "epoch: 28, mean reward: 0.570528332205198\n",
      "epoch: 29, mean reward: 0.5650433190557321\n",
      "epoch: 30, mean reward: 0.54019626560085\n",
      "epoch: 31, mean reward: 0.5282377216928864\n",
      "epoch: 32, mean reward: 0.5051690332420301\n",
      "epoch: 33, mean reward: 0.496442507227067\n",
      "epoch: 34, mean reward: 0.49202174022254325\n",
      "epoch: 35, mean reward: 0.46973909732013747\n",
      "epoch: 36, mean reward: 0.45838706854209293\n",
      "epoch: 37, mean reward: 0.442895599063473\n",
      "epoch: 38, mean reward: 0.42979405856793457\n",
      "epoch: 39, mean reward: 0.42876376393478743\n",
      "epoch: 40, mean reward: 0.4219783837296304\n",
      "epoch: 41, mean reward: 0.4110812673482922\n",
      "epoch: 42, mean reward: 0.408920250918441\n",
      "epoch: 43, mean reward: 0.41598252352893794\n",
      "epoch: 44, mean reward: 0.40641982269974875\n",
      "epoch: 45, mean reward: 0.41374201125359955\n",
      "epoch: 46, mean reward: 0.4099810898925484\n"
     ]
    }
   ],
   "source": [
    "mean_rewards = train_agents(u_agent, v_agent, env, n_epochs, n_sessions, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(mean_rewards)), mean_rewards, '-o')\n",
    "plt.xticks(range(len(mean_rewards)))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Mean reward')\n",
    "plt.title('Mean train rewards over epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test u_gent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_test_agent = CCEMAgent((2,), (1,), percentile_param=70, action_max=env.v_action_max, reward_param=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = train_agents(u_agent, v_test_agent, env, n_epochs, n_sessions, epsilon, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(test_rewards)), test_rewards, '-o')\n",
    "plt.xticks(range(len(test_rewards)))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Mean reward')\n",
    "plt.title('Mean test rewards over epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
