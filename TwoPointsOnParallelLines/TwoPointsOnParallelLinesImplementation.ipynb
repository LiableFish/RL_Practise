{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwoPointsOnParallelLinesImplementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6scIJWSWHn3",
        "colab_type": "text"
      },
      "source": [
        "# Two Points On Parallel Lines solution with CCEM method¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CUx8kogWuoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQk_oJDWWH5C",
        "colab_type": "text"
      },
      "source": [
        "## TODO\n",
        "\n",
        "1) Get best gaurenteed result as 0 for CCEM method\n",
        "\n",
        "2) Try dicsrete Cross-Entropy to improve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvGpn085WS6X",
        "colab_type": "text"
      },
      "source": [
        "## Code implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w8rtmzSWmOU",
        "colab_type": "text"
      },
      "source": [
        "### Evironment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xcs7tjWXzGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoPointsOnParallelLines:\n",
        "    def __init__(self, initial_x=np.array([4.5, 0, 0, 0]), alpha=1, m1=0.01, m2=1, theta=10,\n",
        "                 u_action_max=2.5, v_action_max=1, dt=0.005):\n",
        "        self.initial_x = initial_x\n",
        "        self.alpha = alpha\n",
        "        self.m1 = m1\n",
        "        self.m2 = m2\n",
        "        self.theta = theta\n",
        "        self.u_action_max = u_action_max\n",
        "        self.v_action_max = v_action_max\n",
        "        self.dt = dt\n",
        "        self.state = self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.hstack((0, self.initial_x))\n",
        "        return self.state\n",
        "\n",
        "    def step(self, u_action, v_action):\n",
        "        t, x1, x2, x3, x4 = self.state\n",
        "        x1 = x1 + x2 * self.dt\n",
        "        x2 = x2 + (- (self.alpha / self.m1) * x2 + (1 / self.m1) * u_action) * self.dt\n",
        "        x3 = x3 + x4 * self.dt\n",
        "        x4 = x4 + (1 / self.m2) * v_action * self.dt\n",
        "        t += self.dt\n",
        "        self.state = np.array([t, x1, x2, x3, x4])\n",
        "\n",
        "        reward = 0\n",
        "        done = False\n",
        "        if t >= self.theta:\n",
        "            reward = - np.abs(x1 - x3)\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, None\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKUHJj3CYS1A",
        "colab_type": "text"
      },
      "source": [
        "### CCME Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73FuTlB0Ydjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    \"\"\"Neural network for an agent\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        \"\"\"Create new network\n",
        "        :param input_shape: input data shape\n",
        "        :param output_shape: output data shape\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear_1 = torch.nn.Linear(input_shape[0], 50)\n",
        "        self.linear_2 = torch.nn.Linear(50, 30)\n",
        "        self.linear_3 = torch.nn.Linear(30, output_shape[0])\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.tang = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, input_):\n",
        "        \"\"\"Network step\n",
        "        :param input_: input data\n",
        "        :return: network output result\n",
        "        \"\"\"\n",
        "        hidden = self.relu(self.linear_1(input_))\n",
        "        hidden = self.relu(self.linear_2(hidden))\n",
        "        output = self.tang(self.linear_3(hidden))\n",
        "        return output\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p7ESnPnYY0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CCEMAgent(torch.nn.Module):\n",
        "    \"\"\"Continuous cross-entropy method agent implementation\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, action_shape, action_max, reward_param=1, percentile_param=70, noise_decrease=0.98,\n",
        "                 tau=1e-2, learning_rate=1e-2, n_learning_per_fit=16, mini_batch_size=200):\n",
        "        \"\"\"Create new agent\n",
        "        :param state_shape: environment's state shape\n",
        "        :param action_shape: agent's action shape\n",
        "        :param action_max: maximum action value\n",
        "        :param reward_param: equal to 1 if agent wants to maximize reward otherwise -1\n",
        "        :param percentile_param: percentile to get elite sessions\n",
        "        :param noise_decrease: noise decrease value\n",
        "        :param tau: network weights updating rate\n",
        "        :param learning_rate: learning rate for gradient descent method\n",
        "        :param n_learning_per_fit: number of network updating weights iterations per fit\n",
        "        :param mini_batch_size: count of elements to sample to mini-batch\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.action_max = np.abs(action_max)\n",
        "        self.reward_param = reward_param\n",
        "        self.percentile_param = percentile_param\n",
        "        self.noise_decrease = noise_decrease\n",
        "        self.noise_threshold = 1\n",
        "        self.min_noise_threshold = 0.1\n",
        "        self.tau = tau\n",
        "        self.n_learning_per_fit = n_learning_per_fit\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.network = Network(state_shape, action_shape)\n",
        "        self.optimizer = torch.optim.Adam(params=self.network.parameters(), lr=learning_rate)\n",
        "\n",
        "    def get_action(self, state, test=False):\n",
        "        \"\"\"Get an action by current state\n",
        "        :param state: current environment state\n",
        "        :param test: if True noise will not be added and will be otherwise\n",
        "        :return: predicted action\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state)\n",
        "        predicted_action = self.network(state).detach().numpy() * self.action_max\n",
        "        if not test:\n",
        "            noise = self.noise_threshold * np.random.uniform(low=-self.action_max, high=self.action_max)\n",
        "            predicted_action = np.clip(predicted_action + noise, -self.action_max, self.action_max)\n",
        "        return predicted_action\n",
        "\n",
        "    def get_elite_states_and_actions(self, sessions):\n",
        "        \"\"\"Select sessions with the most or least reward by percentile\n",
        "        :param sessions: list of sessions to choose elite ones from\n",
        "        :return: elite states, elite actions\n",
        "        \"\"\"\n",
        "        total_rewards = [session['total_reward'] for session in sessions]\n",
        "        reward_threshold = np.percentile(total_rewards, self.percentile_param)\n",
        "        \n",
        "        elite_states = []\n",
        "        elite_actions = []\n",
        "        for session in sessions:\n",
        "            if self.reward_param * (session['total_reward'] - reward_threshold) > 0:\n",
        "                elite_states.extend(session['states'])\n",
        "                elite_actions.extend(session[f'{self}actions'])\n",
        "\n",
        "        return torch.FloatTensor(elite_states), torch.FloatTensor(elite_actions)\n",
        "\n",
        "    def learn_network(self, loss):\n",
        "        \"\"\"Update network weights by optimize loss\n",
        "        :param loss: loss function to optimize\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        old_network = deepcopy(self.network)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        for new_parameter, old_parameter in zip(self.network.parameters(), old_network.parameters()):\n",
        "            new_parameter.data.copy_(self.tau * new_parameter + (1 - self.tau) * old_parameter)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def fit(self, sessions):\n",
        "        \"\"\"Fitting process using mini-batches\n",
        "        :param sessions: sessions to fit on\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        elite_states, elite_actions = self.get_elite_states_and_actions(sessions)\n",
        "\n",
        "        for _ in range(self.n_learning_per_fit):\n",
        "            predicted_action = self.network(elite_states) * self.action_max\n",
        "            loss = torch.mean((predicted_action - elite_actions) ** 2)\n",
        "            self.learn_network(loss)\n",
        "\n",
        "        if self.noise_threshold > self.min_noise_threshold:\n",
        "            self.noise_threshold *= self.noise_decrease\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"An agent string representation to define if it is u_agent or v_agent\n",
        "        :return: string representation\n",
        "        \"\"\"\n",
        "        return 'u_' if self.reward_param == -1 else 'v_'\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DBdUDg6YjgF",
        "colab_type": "text"
      },
      "source": [
        "### Generate sessions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGZ5du1tZGkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(first_agent, second_agent, env, n_iter_update, test=False):\n",
        "    \"\"\"Generate session on environment with agents with updating actions each n_iter_update-th time\n",
        "    :param first_agent: first agent (u_agent by default)\n",
        "    :param second_agent: second agent (v_agent by default)\n",
        "    :param env: environment\n",
        "    :param n_iter_update: number of iterations between getting new actions\n",
        "    :param test: if True first agent will not add noise in get_action method\n",
        "    :return: session dict wit states, first agent actions, second agent actions and total rewards\n",
        "    \"\"\"\n",
        "    states = []\n",
        "    first_agent_actions = []\n",
        "    second_agent_actions = []\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    iteration = 0\n",
        "    while not done:\n",
        "        if iteration % n_iter_update == 0:\n",
        "            first_agent_action = first_agent.get_action(state, test=test)\n",
        "            second_agent_action = second_agent.get_action(state)\n",
        "            states.append(state)\n",
        "            first_agent_actions.append(first_agent_action)\n",
        "            second_agent_actions.append(second_agent_action)\n",
        "        actions = (first_agent_action[0], second_agent_action[0]) if str(first_agent) == 'u_' else (second_agent_action[0], first_agent_action[0])\n",
        "        next_state, reward, done, _ = env.step(*actions)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        iteration += 1\n",
        "\n",
        "    return {'states': states,\n",
        "            f'{first_agent}actions': first_agent_actions,\n",
        "            f'{second_agent}actions': second_agent_actions,\n",
        "            'total_reward': total_reward}\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On5JeAyyZQfo",
        "colab_type": "text"
      },
      "source": [
        "### Fit one epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EZs43b-ZTgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_epoch(u_agent, v_agent, env, n_sessions, n_iter_update, test):\n",
        "    \"\"\"Fit agents during an one epoch\n",
        "    :param u_agent: agent that wants to minimize reward\n",
        "    :param v_agent: agent that wants to maximize reward\n",
        "    :param env: environment\n",
        "    :param n_sessions: number of sessions\n",
        "    :param n_iter_update: number of iterations between getting new actions\n",
        "    :param test: if True u_agent will not be fitted\n",
        "    :return: mean total reward over sessions\n",
        "    \"\"\"\n",
        "    #sessions = [generate_session(u_agent, v_agent, env, test=test) for _ in range(n_sessions)]\n",
        "    with torch.multiprocessing.Pool(torch.multiprocessing.cpu_count()) as pool:\n",
        "        sessions = pool.starmap(generate_session, [(u_agent, v_agent, env, n_iter_update, test) for _ in range(n_sessions)])\n",
        "    mean_reward = np.mean([session['total_reward'] for session in sessions])\n",
        "    if not test:\n",
        "        u_agent.fit(sessions)\n",
        "    v_agent.fit(sessions)\n",
        "    return mean_reward"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leqgdFE-ZVli",
        "colab_type": "text"
      },
      "source": [
        "### Test agents\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf-s6CyLaKQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_agent(u_agent, env, n_epochs, n_sessions, n_iter_update, epsilon):\n",
        "    \"\"\"Test u_agent by fit a new v_agent\n",
        "    :param u_agent: agent to test (must be u_agent type)\n",
        "    :param env: environment\n",
        "    :param n_epochs: number of epochs to fit\n",
        "    :param n_sessions: number of sessions for one epoch\n",
        "    :param n_iter_update: number of iterations between getting new actions\n",
        "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
        "    :return: test total rewards\n",
        "    \"\"\"\n",
        "    v_agent = CCEMAgent((5,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)\n",
        "    _, rewards, _ = fit_agents(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update, epsilon, test=True)\n",
        "    return rewards"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kzV4HKqsJ1Sb"
      },
      "source": [
        "### Fit agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GY6XooSIHbeE",
        "colab": {}
      },
      "source": [
        "def fit_agents(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update,\n",
        "               epsilon, n_iter_debug=0, test=False):\n",
        "    \"\"\"Fit both agent together during several epochs\n",
        "    :param u_agent: agent that wants to minimize reward\n",
        "    :param v_agent: agent that wants to maximize reward\n",
        "    :param env: environment\n",
        "    :param n_epochs: number of epochs to fit\n",
        "    :param n_sessions: number of sessions for one epoch\n",
        "        :param n_iter_update: number of iterations between getting new actions\n",
        "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
        "    :param n_iter_debug: number of iteration between tests\n",
        "    :param test: if True u_agent will not be fitted\n",
        "    :return: u_agent, mean total rewards, test total rewards\n",
        "    \"\"\"\n",
        "    last_mean_reward = 0\n",
        "    mean_rewards = []\n",
        "    test_rewards = []\n",
        "    epoch = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        mean_reward = fit_epoch(u_agent, v_agent, env, n_sessions=n_sessions, n_iter_update=n_iter_update, test=test)\n",
        "        mean_rewards.append(mean_reward)\n",
        "        print(f'epoch: {epoch}, mean reward: {mean_reward}')\n",
        "        if np.abs(last_mean_reward - mean_reward) < epsilon:\n",
        "            break\n",
        "        last_mean_reward = mean_reward\n",
        "\n",
        "        if n_iter_debug and (epoch + 1) % n_iter_debug == 0:\n",
        "            print('\\n{:-^50}\\n'.format('TEST BEGIN'))\n",
        "            test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=epsilon))\n",
        "            print('\\n{:-^50}\\n'.format('TEST END'))             \n",
        "\n",
        "    return u_agent, np.array(mean_rewards), np.array(test_rewards)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ts0iEOAPwNbf"
      },
      "source": [
        "### Fit agents one by one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6zWWu0hLwXVU",
        "colab": {}
      },
      "source": [
        "def fit_agents_one_by_one(u_agent, v_agent, env, n_epochs, n_sessions, n_iter_update,\n",
        "                          n_iter_for_fit, epsilon, n_iter_debug=0):\n",
        "    \"\"\"Fit agents ony by one during several epochs.\n",
        "    During fix number of iterations one agent will be fitted while the other will not.\n",
        "    :param u_agent: agent that wants to minimize reward\n",
        "    :param v_agent: agent that wants to maximize reward\n",
        "    :param env: environment\n",
        "    :param n_epochs: number of epochs to fit\n",
        "    :param n_sessions: number of sessions for one epoch\n",
        "    :param n_iter_update: number of iterations between getting new actions\n",
        "    :param n_iter_for_fit: number of iterations between agent switching\n",
        "    :param epsilon: early stopping criterion (-1 to use all epochs)\n",
        "    :param n_iter_debug: number of iteration between tests\n",
        "    :return: u_agent, mean  total rewards, test total rewards\n",
        "    \"\"\"\n",
        "    last_mean_reward = 0\n",
        "    mean_rewards = []\n",
        "    test_rewards = []\n",
        "    fitting_agent = u_agent\n",
        "    awaiting_agent = v_agent\n",
        "    epoch = 0\n",
        "    stop = False\n",
        "\n",
        "    while not stop and epoch < n_epochs:\n",
        "\n",
        "        for _ in range(n_iter_for_fit):\n",
        "\n",
        "            mean_reward = fit_epoch(awaiting_agent, fitting_agent, env, n_sessions=n_sessions, n_iter_update=n_iter_update, test=True)\n",
        "            mean_rewards.append(mean_reward)\n",
        "            print(f'epoch: {epoch}, current agent: {fitting_agent}, mean reward: {mean_reward}')\n",
        "            if np.abs(last_mean_reward - mean_reward) < epsilon:\n",
        "                stop = True\n",
        "                break\n",
        "            last_mean_reward = mean_reward\n",
        "\n",
        "            if n_iter_debug and (epoch + 1) % n_iter_debug == 0:\n",
        "                print('\\n{:-^50}\\n'.format('TEST BEGIN'))\n",
        "                test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=epsilon))\n",
        "                print('\\n{:-^50}\\n'.format('TEST END'))\n",
        "\n",
        "            epoch += 1\n",
        "            if epoch >= n_epochs:\n",
        "                break\n",
        "\n",
        "        print('\\n')\n",
        "        awaiting_agent, fitting_agent = fitting_agent, awaiting_agent\n",
        "\n",
        "    return u_agent, np.array(mean_rewards), np.array(test_rewards)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VY5KIG4n5qAi"
      },
      "source": [
        "### Fit random agent pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "To2h6mLx7cSE",
        "colab": {}
      },
      "source": [
        "def fit_random_agent_pairs(u_agents, v_agents, env, n_pairs, n_epochs, n_sessions, n_iter_update, n_iter_debug=0):\n",
        "    \"\"\" Fit random pairs of u_ and v_agents\n",
        "    :param u_agent: agent that wants to minimize reward\n",
        "    :param v_agent: agent that wants to maximize reward\n",
        "    :param env: environment\n",
        "    :param n_pairs: number of pairs to fit\n",
        "    :param n_epochs: number of epochs for one pair fit\n",
        "    :param n_sessions: number of sessions for one epoch\n",
        "    :param n_iter_update: number of iterations between getting new actions\n",
        "    :param n_iter_debug: number of iteration between tests\n",
        "    :return: u_agent that will have minimum test total reward, mean total rewards for u_agents, test total rewards\n",
        "    \"\"\"\n",
        "    u_agents_mean_rewards = [[] for _ in range(len(u_agents))]\n",
        "    test_rewards = []\n",
        "\n",
        "    for i in range(n_pairs):\n",
        "        u_agent_idx = np.random.choice(len(u_agents))\n",
        "        v_agent_idx = np.random.choice(len(v_agents))\n",
        "        print(f'PAIR {i + 1} OF {n_pairs}')\n",
        "        print('\\n{:-^50}\\n'.format(f'U_AGENT_{u_agent_idx} VS V_AGENT_{v_agent_idx}'))\n",
        "        _, mean_rewards, _ = fit_agents(u_agents[u_agent_idx], v_agents[v_agent_idx],\n",
        "                                        env=env, n_epochs=n_epochs, n_sessions=n_sessions,\n",
        "                                        n_iter_update=n_iter_update, epsilon=-1, n_iter_debug=0)\n",
        "        print('\\n{:-^50}\\n'.format(''))\n",
        "\n",
        "        u_agents_mean_rewards[u_agent_idx].append(mean_rewards.min())\n",
        "\n",
        "    for i, u_agent in enumerate(u_agents):\n",
        "        print(f'\\nTESTING U_AGENT_{i}\\n')\n",
        "        test_rewards.append(test_agent(u_agent, env, n_epochs=300, n_sessions=n_sessions, n_iter_update=n_iter_update, epsilon=-1))\n",
        "\n",
        "    best_u_agent_idx = np.argmin([test.max() for test in test_rewards])\n",
        "    print(f'\\nBest agent is {best_u_agent_idx}, its test reward is {np.min(test_rewards[best_u_agent_idx])}\\n')\n",
        "    return u_agents[best_u_agent_idx], np.array(u_agents_mean_rewards), np.array(test_rewards)\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4wR_cLRa2r7A"
      },
      "source": [
        "### Plot mean rewards by epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pI2rrGrK2w6l",
        "colab": {}
      },
      "source": [
        "def plot_mean_rewards(mean_rewards, method_name, axes, test_rewards=None):\n",
        "    \"\"\"Plot mean rewards\n",
        "    :param mean_rewards: rewards to plot\n",
        "    :param method_name: fitting method name\n",
        "    :param test_rewards: test rewards to plot\n",
        "    \"\"\"\n",
        "    if test_rewards is not None:\n",
        "        ax1, ax2 = axes\n",
        "    else:\n",
        "        ax1 = axes\n",
        "    ax1.plot(range(len(mean_rewards)), mean_rewards)\n",
        "    ax1.set_xlabel('Номер эпохи')\n",
        "    ax1.set_ylabel('Средний total reward')\n",
        "    ax1.set_title(f'Среднее значение total reward для {method_name}')\n",
        "    ax1.grid()\n",
        "    if test_rewards is not None:\n",
        "        ax2.set_xlabel('Номер эпохи')\n",
        "        ax2.set_ylabel('Тестовый total reward')\n",
        "        ax2.set_title(f'Тестовое значение total reward для {method_name}')\n",
        "        for i, test_reward in enumerate(test_rewards):\n",
        "            ax2.plot(range(len(test_reward)), test_reward, label=f'После {int((i + 1) * len(mean_rewards) / len(test_rewards))} эпох')\n",
        "        ax2.legend(loc='upper left')\n",
        "        ax2.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeMpUBgT6_c",
        "colab_type": "text"
      },
      "source": [
        "### Testing multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz8O6xSXb0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = TwoPointsOnParallelLines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4n1hLgNT6_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "u_agent = CCEMAgent((5,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
        "v_agent = CCEMAgent((5,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dh-XFmT6_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fd74cd1-9f94-46cd-8292-bf3ab39ed9f3"
      },
      "source": [
        "%%timeit\n",
        "sessions = [generate_session(u_agent, v_agent, env, 100, test=False) for _ in range(100)]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 3.46 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3RXEMQRT6_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = generate_session(u_agent, v_agent, env, 100, test=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlAeLW5AT6_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "68eb309a-de1c-489b-f474-a9a828e5f0c3"
      },
      "source": [
        "len(s['states'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY1dsAKeT6_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ef-s8ZT6_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a03e1849-6117-4d27-e35e-e1658b5ab501"
      },
      "source": [
        "process_count = multiprocessing.cpu_count()\n",
        "process_count"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJYwul3ST6_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f110bbd4-d15e-453c-989f-41e65fa8a254"
      },
      "source": [
        "with torch.multiprocessing.Pool(process_count) as pool:\n",
        "    %timeit pool.starmap(generate_session, [(u_agent, v_agent, env, 100, False) for _ in range(100)])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 3.38 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pgUptSYv2FIK"
      },
      "source": [
        "## Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHjDLywkb3ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = TwoPointsOnParallelLines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LDwDnGJD5qa9"
      },
      "source": [
        "### Deafualt fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EXE4PAqKJ1Se",
        "colab": {}
      },
      "source": [
        "u_agent = CCEMAgent((5,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
        "v_agent = CCEMAgent((5,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_bpTd8JerT8E",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "u_fit_agent, mean_rewards, test_rewards \\\n",
        "= fit_agents(u_agent, v_agent, env, n_epochs=1000, n_sessions=100, n_iter_update=20, epsilon=-1, n_iter_debug=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "orw6Xmrgrdww",
        "colab": {}
      },
      "source": [
        "_, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "plot_mean_rewards(mean_rewards, method_name='CCME метода', axes=axes, test_rewards=test_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rsIq2Y3V5njb"
      },
      "source": [
        "### Fit one by one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OW1QxXVg6gYp",
        "colab": {}
      },
      "source": [
        "u_agent_one_by_one = CCEMAgent((5,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\n",
        "v_agent_one_by_one = CCEMAgent((5,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4DV2oVjprieg",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "u_fit_agent_one_by_one, mean_rewards_one_by_one, test_rewards_one_by_one =\\\n",
        "fit_agents_one_by_one(u_agent_one_by_one, v_agent_one_by_one, env, \\\n",
        "                       n_epochs=1000, n_sessions=100, n_iter_for_fit=50, n_iter_update=20, epsilon=-1, n_iter_debug=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eydMMs5KryCi",
        "colab": {}
      },
      "source": [
        "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "c = 'b'\n",
        "l = 0\n",
        "\n",
        "for rewards in np.array_split(mean_rewards_one_by_one, 20):\n",
        "    ax1.plot(range(l, l + len(rewards)), rewards, color=c)\n",
        "    l = l + len(rewards)\n",
        "    c = 'b' if c=='g' else 'g'\n",
        "ax1.set_xlabel('Номер эпохи')\n",
        "ax1.set_ylabel('Средний total reward')\n",
        "ax1.set_title('Среднее значение total reward\\nдля метода попеременного обучения')\n",
        "ax1.legend(labels=['u_agent', 'v_agent'])\n",
        "ax1.grid()\n",
        "\n",
        "ax2.set_xlabel('Номер эпохи')\n",
        "ax2.set_ylabel('Тестовый total reward')\n",
        "ax2.set_title('Тестовое значение total reward\\nдля метода попеременного обучения')\n",
        "\n",
        "for i, test_reward in enumerate(test_rewards_one_by_one):\n",
        "    ax2.plot(range(len(test_reward)), test_reward, label=f'После {int((i + 1) * len(mean_rewards_one_by_one) / len(test_rewards_one_by_one))} эпох')\n",
        "ax2.legend(loc='upper left')\n",
        "ax2.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "348CMrbE5U5Y"
      },
      "source": [
        "## Fit random pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZQDoMwYUcf_",
        "colab": {}
      },
      "source": [
        "u_agents = [CCEMAgent((5,), (1,), percentile_param=10, action_max=env.u_action_max, reward_param=-1)\\\n",
        "            for _ in range(5)]\n",
        "v_agents = [CCEMAgent((5,), (1,), percentile_param=90, action_max=env.v_action_max, reward_param=1)\\\n",
        "            for _ in range(5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lgcN3yChr2CW",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "u_fit_random_agent, mean_rewards_random, test_rewards_random = \\\n",
        "fit_random_agent_pairs(u_agents, v_agents, env,\\\n",
        "                       n_pairs=20, n_epochs=50, n_sessions=100,\\\n",
        "                       n_iter_update=20, n_iter_debug=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBU50PYPsb_W",
        "colab": {}
      },
      "source": [
        "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "for i, rewards in enumerate(mean_rewards_random):\n",
        "    ax1.plot(range(len(rewards)), rewards, label=f'u_agent_{i}')\n",
        "\n",
        "for i, test in enumerate(test_rewards_random):\n",
        "    ax2.plot(range(len(test)), test, label=f'u_agent_{i}')\n",
        "\n",
        "ax1.set_xlabel('Номер пары')\n",
        "ax1.set_ylabel('Средний total reward')\n",
        "ax1.set_title('Среднее значение total reward\\nдля метода случайных пар')\n",
        "ax1.grid()\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "ax2.set_xlabel('Номер эпохи')\n",
        "ax2.set_ylabel('Тестовый total reward')\n",
        "ax2.set_title('Тестовое значение total reward\\nдля каждого агента')\n",
        "ax2.grid()\n",
        "ax2.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a9PYm_lX2h-H"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MM6uZqenAZoj"
      },
      "source": [
        "Getting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Zfw7hoQAbna",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Q4q2qUFO1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def highlight_min(s):\n",
        "    '''\n",
        "    highlight the minimum in a Series yellow.\n",
        "    '''\n",
        "    is_min = s == s.min()\n",
        "    return ['background-color: yellow' if v else '' for v in is_min]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZqdAcMD1pOpi",
        "colab": {}
      },
      "source": [
        "def get_test_data(test_rewards, test_rewards_one_by_one, test_rewards_random):\n",
        "    data = np.vstack((test_rewards.max(axis=1),\\\n",
        "                      test_rewards_one_by_one.max(axis=1),\\\n",
        "                     test_rewards_random.max(axis=1)))\n",
        "    return pd.DataFrame(data=data,\\\n",
        "                        columns=[f'После {i} эпох' for i in range(200, 1001, 200)],\\\n",
        "                        index=['CCME', 'Метод отложенного обучения', 'Метод случайных пар'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GH1TID87ogDz",
        "colab": {}
      },
      "source": [
        "dt01_data = get_test_data(test_rewards, test_rewards_one_by_one, test_rewards_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZaNX_lBpmLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "51a98f24-af6e-416f-b415-979926cffada"
      },
      "source": [
        "display(dt01_data.style.apply(highlight_min, axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "    #T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col4 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col3 {\n",
              "            background-color:  yellow;\n",
              "        }    #T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col3 {\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >После 200 эпох</th>        <th class=\"col_heading level0 col1\" >После 400 эпох</th>        <th class=\"col_heading level0 col2\" >После 600 эпох</th>        <th class=\"col_heading level0 col3\" >После 800 эпох</th>        <th class=\"col_heading level0 col4\" >После 1000 эпох</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >CCME</th>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.754725</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.465713</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.405415</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.094068</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0.057352</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Метод отложенного обучения</th>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col0\" class=\"data row1 col0\" >9.110382</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col1\" class=\"data row1 col1\" >11.593006</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.781109</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.184886</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0.217717</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >Метод случайных пар</th>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col0\" class=\"data row2 col0\" >7.059424</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.611673</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col2\" class=\"data row2 col2\" >2.273378</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.232129</td>\n",
              "                        <td id=\"T_c7acfd08_bd40_11ea_a1fe_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0.526885</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb9b58e2390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}